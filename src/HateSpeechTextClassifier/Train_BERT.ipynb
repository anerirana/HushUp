{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"TextClassifier_BERT_Public.ipynb","provenance":[{"file_id":"1NWWMXhAfdl16PZ-2nfeKVDw3Pb7KJTUd","timestamp":1643523269944},{"file_id":"1XwO7x2aaMwNupsV-z-xy4QWG5HvFOIRC","timestamp":1632900550678}],"collapsed_sections":["4dFSOXNx1R18","SiyaeHm51WES","xs1VS-XGViQy"]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"4dFSOXNx1R18"},"source":["# Prepapre Env"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"R2Q8KHpniNMP"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mthZxVsUChvR"},"source":["!pip install torch torchvision\n","!pip install transformers"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SiyaeHm51WES"},"source":["# Prepare Workspace"]},{"cell_type":"code","metadata":{"id":"sSunOm4IMH76"},"source":["import torch\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from keras.preprocessing.sequence import pad_sequences\n","from transformers import BertTokenizer, BertConfig\n","from transformers import AdamW, BertForSequenceClassification\n","from transformers import get_linear_schedule_with_warmup\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import ConfusionMatrixDisplay\n","\n","from tqdm import trange\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","% matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"WC-Ehc8jMgyN","executionInfo":{"status":"ok","timestamp":1633020965281,"user_tz":-330,"elapsed":21,"user":{"displayName":"aneri rana","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00984782061859407690"}},"outputId":"ee8b08cf-4e2c-409a-d904-ce7848fdeeb3"},"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","n_gpu = torch.cuda.device_count()\n","torch.cuda.get_device_name(0)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Tesla K80'"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"6Z3txzDOVY9A"},"source":["WORKSPACE = \"drive/MyDrive/HushUp/OffensiveTextClassifier\"\n","DATA_FILE = WORKSPACE + \"/text_data.csv\"\n","MODEL_DIR = \"bert-base-uncased\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xs1VS-XGViQy"},"source":["# Data Preprocessing"]},{"cell_type":"code","metadata":{"id":"ZyjsHOF9VxOT"},"source":["df = pd.read_csv(DATA_FILE)\n","print(df.shape)\n","print(df.sample(5))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nz-n7kxAUA2o"},"source":["# Create sentence and label lists\n","sentences = np.array(df['Text'])\n","\n","# Add special tokens at the beginning and end of each sentence\n","sentences = [\"[CLS] \" + str(sentence) + \" [SEP]\" for sentence in sentences]\n","labels = np.array(df['Label'])\n","labels = [int(label) for label in labels]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x2kWw8Ph-8_y","executionInfo":{"status":"ok","timestamp":1633021363333,"user_tz":-330,"elapsed":375,"user":{"displayName":"aneri rana","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00984782061859407690"}},"outputId":"156b1e39-a4ed-45c7-97c0-0771aa8af4a1"},"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n","tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","print(\"Tokenize the first sentence:\")\n","print(tokenized_texts[0])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenize the first sentence:\n","['[CLS]', 'do', 'jews', 'wear', 'those', 'hats', 'to', 'cover', 'their', 'horns', '?', '[SEP]']\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HrbA_FzKcXJJ","executionInfo":{"status":"ok","timestamp":1633021370244,"user_tz":-330,"elapsed":371,"user":{"displayName":"aneri rana","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00984782061859407690"}},"outputId":"9e3213c1-90f4-44c2-ca96-b215edb125cc"},"source":["# Set the maximum sequence length.\n","MAX_LEN = 128\n","\n","# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n","input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","\n","# Pad our input tokens\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","print(input_ids[0])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[  101  2079  5181  4929  2216 16717  2000  3104  2037 11569  1029   102\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0]\n"]}]},{"cell_type":"code","metadata":{"id":"TG7kU0Ylb9fJ"},"source":["# Create attention masks\n","attention_masks = []\n","\n","# Create a mask of 1s for each token followed by 0s for padding\n","for seq in input_ids:\n","  seq_mask = [float(i>0) for i in seq]\n","  attention_masks.append(seq_mask)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EB2Lqh10lbC6"},"source":["# Use train_test_split to split our data into train and validation sets for training\n","train_inputs, temp_inputs, train_labels, temp_labels = train_test_split(input_ids, labels, \n","                                                            random_state=2018, test_size=0.2)\n","test_inputs, validation_inputs, test_labels, validation_labels = train_test_split(temp_inputs, temp_labels, \n","                                                            random_state=2018, test_size=0.5)\n","\n","train_masks, temp_masks, _, _ = train_test_split(attention_masks, input_ids,\n","                                             random_state=2018, test_size=0.2)\n","test_masks, validation_masks, _, _ = train_test_split(temp_masks, temp_inputs,\n","                                             random_state=2018, test_size=0.5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jM5g_rdO9efn"},"source":["print(\"Dataset distribution\")\n","print(\"train\\n\", np.unique(train_labels, return_counts=True))\n","print(\"validation\\n\",np.unique(validation_labels, return_counts=True))\n","print(\"test\\n\",np.unique(test_labels, return_counts=True))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gr6331fCkhqF"},"source":["# Convert all of our data into torch tensors, the required datatype for our model\n","train_inputs = torch.tensor(train_inputs)\n","validation_inputs = torch.tensor(validation_inputs)\n","train_labels = torch.tensor(train_labels)\n","validation_labels = torch.tensor(validation_labels)\n","train_masks = torch.tensor(train_masks)\n","validation_masks = torch.tensor(validation_masks)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jCpzJPqJ1h0e"},"source":["# Train Model"]},{"cell_type":"code","metadata":{"id":"wr0iS2spv1R_"},"source":["# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n","model = BertForSequenceClassification.from_pretrained(MODEL_DIR)\n","model.cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5ToEBfnkutak"},"source":["# Select a batch size for training\n","batch_size = 4\n","\n","# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n","# with an iterator the entire dataset does not need to be loaded into memory\n","\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","validation_sampler = SequentialSampler(validation_data)\n","validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7T6VI6d5xtuT"},"source":["param_optimizer = list(model.named_parameters())\n","no_decay = ['bias', 'LayerNorm.weight']\n","# This variable contains all of the hyperparemeter information our training loop needs\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n","     'weight_decay_rate': 0.01},\n","    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n","     'weight_decay_rate': 0.0}\n","]\n","\n","# Parameters:\n","lr =  5e-6\n","max_grad_norm = 1.0\n","\n","# Number of training epochs (authors recommend between 2 and 4)\n","epochs = 6\n","\n","num_training_steps = epochs * len(train_dataloader)\n","num_warmup_steps = 2000"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HgOfa7MYx8wj"},"source":["optimizer = AdamW(optimizer_grouped_parameters, lr=lr, correct_bias=False)\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2bpimDu8z2U5"},"source":["# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q3MJ51B91dQE"},"source":["t = [] \n","\n","# Store our loss and accuracy for plotting\n","train_loss_set = []\n","val_loss_set = []\n","\n","# trange is a tqdm wrapper around the normal python range\n","for _ in trange(epochs, desc=\"Epoch\"):\n","  # Training\n","  # Set our model to training mode (as opposed to evaluation mode)\n","  model.train()\n","  \n","  # Tracking variables\n","  tr_loss = 0\n","  nb_tr_examples, nb_tr_steps = 0, 0\n","  \n","  # Train the data for one epoch\n","  for step, batch in enumerate(train_dataloader):\n","    # Add batch to GPU\n","    batch = tuple(t.to(device) for t in batch)\n","    # Unpack the inputs from our dataloader\n","    b_input_ids, b_input_mask, b_labels = batch\n","    # Clear out the gradients (by default they accumulate)\n","    optimizer.zero_grad()\n","    # Forward pass\n","    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n","    loss = outputs.loss\n","    train_loss_set.append(loss.item())    \n","    # Backward pass\n","    loss.backward()\n","    # Update parameters and take a step using the computed gradient\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)  # Gradient clipping is not in AdamW anymore (so you can use amp without issue)\n","    optimizer.step()\n","    scheduler.step() \n","    \n","    # Update tracking variables\n","    tr_loss += loss.item()\n","    nb_tr_examples += b_input_ids.size(0)\n","    nb_tr_steps += 1\n","\n","  print(\"Train loss: {}\".format(tr_loss/len(train_dataloader)))\n","    \n","    \n","  # Validation\n","  # Put model in evaluation mode to evaluate loss on the validation set\n","  model.eval()\n","\n","  # Tracking variables \n","  eval_loss, eval_accuracy = 0, 0\n","  nb_eval_steps, nb_eval_examples = 0, 0\n","\n","  # Evaluate data for one epoch\n","  for batch in validation_dataloader:\n","    # Add batch to GPU\n","    batch = tuple(t.to(device) for t in batch)\n","    # Unpack the inputs from our dataloader\n","    b_input_ids, b_input_mask, b_labels = batch\n","    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n","    with torch.no_grad():\n","      # Forward pass, calculate logit predictions\n","      outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n","      logits = outputs.logits\n","      loss = outputs.loss\n","      val_loss_set.append(loss.item()) \n","    \n","    # Move logits and labels to CPU\n","    logits = logits.detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","\n","    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","    \n","    eval_accuracy += tmp_eval_accuracy\n","    nb_eval_steps += 1\n","    eval_loss += loss.item()\n","  print(\"Validation loss: {}\".format(eval_loss/len(validation_dataloader)))\n","  print(\"Validation Accuracy: {}\".format(eval_accuracy/len(validation_dataloader)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mblwTMIL3msX"},"source":["model.save_pretrained(WORKSPACE + \"/Models/BERT_5\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GxrsTdqR1nuI"},"source":["# Evaluate Model"]},{"cell_type":"code","metadata":{"id":"C6gwsasuypPX"},"source":["plt.figure(figsize=(15,8))\n","plt.title(\"Training loss\")\n","plt.xlabel(\"Batch\")\n","plt.ylabel(\"Loss\")\n","plt.plot(train_loss_set, 'b-')\n","plt.plot(val_loss_set, 'g-')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XkF6ggipiiFl"},"source":["prediction_inputs = torch.tensor(test_inputs)\n","prediction_masks = torch.tensor(test_masks)\n","prediction_labels = torch.tensor(test_labels)  \n","\n","prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FIxiazIfjvTQ","executionInfo":{"status":"ok","timestamp":1633021641380,"user_tz":-330,"elapsed":479,"user":{"displayName":"aneri rana","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00984782061859407690"}},"outputId":"4380a09c-d5b3-46b1-8ee3-cd2887770fa6"},"source":["# Prediction on test set\n","# Put model in evaluation mode\n","model.eval()\n","model.cuda()\n","\n","# Tracking variables \n","predictions , true_labels = [], []\n","\n","tmp_test_accuracy = 0\n","test_accuracy = 0\n","nb_test_steps = 0\n","\n","# Predict \n","for batch in prediction_dataloader:\n","  # Add batch to GPU\n","  batch = tuple(t.to(device) for t in batch)\n","  # Unpack the inputs from our dataloader\n","  b_input_ids, b_input_mask, b_labels = batch\n","  # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n","  with torch.no_grad():\n","    # Forward pass, calculate logit predictions\n","    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n","    logits = outputs[0]\n","\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  \n","  # Store predictions and true labels\n","  predictions.append(logits)\n","  true_labels.append(label_ids)\n","\n","  tmp_test_accuracy = flat_accuracy(logits, label_ids)\n","    \n","  test_accuracy += tmp_test_accuracy \n","  nb_test_steps += 1\n","\n","print(\"Testing Accuracy: {}\".format(test_accuracy/nb_test_steps))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Testing Accuracy: 0.8269230769230769\n"]}]},{"cell_type":"markdown","metadata":{"id":"aHtzZFAH2FVL"},"source":["Matthew Scores"]},{"cell_type":"code","metadata":{"id":"UWeWxbB1kVn9"},"source":["# Import and evaluate each test batch using Matthew's correlation coefficient\n","from sklearn.metrics import matthews_corrcoef\n","matthews_set = []\n","\n","for i in range(len(true_labels)):\n","  matthews = matthews_corrcoef(true_labels[i],\n","                 np.argmax(predictions[i], axis=1).flatten())\n","  matthews_set.append(matthews)\n","\n","# Flatten the predictions and true values for aggregate Matthew's evaluation on the whole dataset\n","flat_predictions = [item for sublist in predictions for item in sublist]\n","flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n","flat_true_labels = [item for sublist in true_labels for item in sublist]\n","print(\"Matthew Score: \",matthews_corrcoef(flat_true_labels, flat_predictions))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9k1HhMH7dkgV"},"source":["# Classification report\n","report = classification_report(flat_true_labels, flat_predictions, zero_division=0)\n","print(report)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V6cQ_8apdljl"},"source":["# Confusion Matrix\n","cm = confusion_matrix(flat_true_labels, flat_predictions, labels=[0,1])\n","ConfusionMatrixDisplay(cm, display_labels=[\"non-offensive\",\"offensive\"]).plot(values_format='')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AQVL7E36H58Y"},"source":["# Check False Positives\n","df = pd.DataFrame()\n","df['predictions'] = flat_predictions\n","df['correct_labels'] = flat_true_labels\n","print(df.groupby(df['predictions']).count())\n","print(df.groupby(df['correct_labels']).count())\n","incorrect_predictions = df[df['predictions'] != df['correct_labels']]\n","\n","fp = incorrect_predictions[incorrect_predictions['predictions'] == 1].index.values\n","fn = incorrect_predictions[incorrect_predictions['predictions'] == 0].index.values\n","\n","print(\"#false positives: \" + str(len(fp)))\n","print(\"#false negatives \" + str(len(fn)))\n","\n","np.array(sentences)[fn]"],"execution_count":null,"outputs":[]}]}