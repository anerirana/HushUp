{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"TextClassifier_ALBERT_Public.ipynb","provenance":[{"file_id":"1lPlXb7PHhwVH0EF4t3gd-dT313qK3wX6","timestamp":1643524282677},{"file_id":"1XwO7x2aaMwNupsV-z-xy4QWG5HvFOIRC","timestamp":1603470242024}],"collapsed_sections":["HmahZw8csbqk","zxIHTWjMsgeC","ZotpFuMQ75JS","7JtQBREwttkL"]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","source":["# Set up Env"],"metadata":{"id":"HmahZw8csbqk"}},{"cell_type":"code","metadata":{"id":"mthZxVsUChvR"},"source":["!pip install torch torchvision\n","!pip install transformers\n","!pip install sentencepiece"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ian9vRZ9wH2c"},"source":["# Mount google drive and store the requirements file of installed packages\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Prepare Workspace"],"metadata":{"id":"zxIHTWjMsgeC"}},{"cell_type":"code","metadata":{"id":"sSunOm4IMH76"},"source":["import torch\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from keras.preprocessing.sequence import pad_sequences\n","from transformers import AlbertTokenizer, AlbertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import ConfusionMatrixDisplay\n","\n","import io\n","from tqdm import tqdm, trange\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","% matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WC-Ehc8jMgyN"},"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","n_gpu = torch.cuda.device_count()\n","torch.cuda.get_device_name(0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o5QkWsXZ7P4_"},"source":["WORKSPACE = \"drive/MyDrive/HushUp/OffensiveTextClassifier\"\n","DATA_FILE = WORKSPACE + \"/text_data.csv\"\n","MODEL_DIR = 'albert-base-v2' # 'albert-large-v2'\n","tokenizer_model_name = 'albert-base-v2' # 'albert-large-v2'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZotpFuMQ75JS"},"source":["# Data Preprocessing"]},{"cell_type":"code","metadata":{"id":"XZEW7cqAS7eL"},"source":["df = pd.read_csv(DATA_FILE)\n","print(df.shape)\n","print(df.sample(5))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nz-n7kxAUA2o"},"source":["# Create sentence and label lists\n","sentences = np.array(df['tweet'])\n","\n","# Add special tokens at the beginning and end of each sentence\n","sentences = [\"[CLS] \" + str(sentence) + \" [SEP]\" for sentence in sentences]\n","labels = np.array(df['class'])\n","labels = [int(label) for label in labels]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a4IwqzBOTJDf"},"source":["tokenizer = AlbertTokenizer.from_pretrained(tokenizer_model_name)\n","tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","print (\"Tokenize the first sentence:\")\n","print (tokenized_texts[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bb9tMzlHn_mO"},"source":["# Set the maximum sequence length.\n","MAX_LEN = 128\n","\n","# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n","input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","\n","# Pad our input tokens\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","print(input_ids[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"thMNmuXSoPEs"},"source":["# Create attention masks\n","attention_masks = []\n","\n","# Create a mask of 1s for each token followed by 0s for padding\n","for seq in input_ids:\n","  seq_mask = [float(i>0) for i in seq]\n","  attention_masks.append(seq_mask)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EB2Lqh10lbC6"},"source":["# Use train_test_split to split our data into train and validation sets for training\n","train_inputs, temp_inputs, train_labels, temp_labels = train_test_split(input_ids, labels, \n","                                                            random_state=2018, test_size=0.2)\n","test_inputs, validation_inputs, test_labels, validation_labels = train_test_split(temp_inputs, temp_labels, \n","                                                            random_state=2018, test_size=0.5)\n","\n","train_masks, temp_masks, _, _ = train_test_split(attention_masks, input_ids,\n","                                             random_state=2018, test_size=0.2)\n","test_masks, validation_masks, _, _ = train_test_split(temp_masks, temp_inputs,\n","                                             random_state=2018, test_size=0.5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pWjBDNmV82fK"},"source":["print(\"Dataset distribution\")\n","print(\"train\\n\", np.unique(train_labels, return_counts=True))\n","print(\"validation\\n\",np.unique(validation_labels, return_counts=True))\n","print(\"test\\n\",np.unique(test_labels, return_counts=True))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bBsqkASslVvT"},"source":["# Convert all of our data into torch tensors, the required datatype for our model\n","train_inputs = torch.tensor(train_inputs)\n","validation_inputs = torch.tensor(validation_inputs)\n","train_labels = torch.tensor(train_labels)\n","validation_labels = torch.tensor(validation_labels)\n","train_masks = torch.tensor(train_masks)\n","validation_masks = torch.tensor(validation_masks)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Train Model"],"metadata":{"id":"7JtQBREwttkL"}},{"cell_type":"code","metadata":{"id":"5ToEBfnkutak"},"source":["# Select a batch size for training\n","BATCH_SIZE = 4\n","\n","# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n","# with an iterator the entire dataset does not need to be loaded into memory\n","\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n","\n","validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","validation_sampler = SequentialSampler(validation_data)\n","validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=BATCH_SIZE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S_-gGhstdxdO"},"source":["# Collective Parameters\n","lr =  5e-6\n","max_grad_norm = 1.0\n","\n","# Number of training epochs (authors recommend between 2 and 4)\n","epochs = 6\n","\n","num_training_steps = epochs * len(train_dataloader)\n","num_warmup_steps = 1000"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wr0iS2spv1R_"},"source":["# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n","model = AlbertForSequenceClassification.from_pretrained(MODEL_DIR)\n","model.cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HgOfa7MYx8wj"},"source":["# All of the hyperparemeter information our training loop needs\n","param_optimizer = list(model.named_parameters())\n","no_decay = ['bias', 'gamma', 'beta']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n","     'weight_decay_rate': 0.01},\n","    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n","     'weight_decay_rate': 0.0}\n","]\n","\n","optimizer = AdamW(optimizer_grouped_parameters, lr=lr, correct_bias=False)\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2bpimDu8z2U5"},"source":["# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NQhGhnKOit-N"},"source":["t = [] \n","\n","# Store our loss and accuracy for plotting\n","train_loss_set = []\n","val_loss_set = []\n","\n","# trange is a tqdm wrapper around the normal python range\n","for _ in trange(epochs, desc=\"Epoch\"):\n","  # Training\n","  # Set our model to training mode (as opposed to evaluation mode)\n","  model.train()\n","  \n","  # Tracking variables\n","  tr_loss = 0\n","  nb_tr_examples, nb_tr_steps = 0, 0\n","  \n","  # Train the data for one epoch\n","  for step, batch in enumerate(train_dataloader):\n","    # Add batch to GPU\n","    batch = tuple(t.to(device) for t in batch)\n","    # Unpack the inputs from our dataloader\n","    b_input_ids, b_input_mask, b_labels = batch\n","    # Clear out the gradients (by default they accumulate)\n","    optimizer.zero_grad()\n","    # Forward pass\n","    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n","    loss = outputs.loss\n","    train_loss_set.append(loss.item())    \n","    # Backward pass\n","    loss.backward()\n","    # Update parameters and take a step using the computed gradient\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)  # Gradient clipping is not in AdamW anymore (so you can use amp without issue)\n","    optimizer.step()\n","    scheduler.step()   \n","    \n","    # Update tracking variables\n","    tr_loss += loss.item()\n","    nb_tr_examples += b_input_ids.size(0)\n","    nb_tr_steps += 1\n","\n","  print(\"Train loss: {}\".format(tr_loss/len(train_dataloader)))\n","    \n","    \n","  # Validation\n","  # Put model in evaluation mode to evaluate loss on the validation set\n","  model.eval()\n","\n","  # Tracking variables \n","  eval_loss, eval_accuracy = 0, 0\n","  nb_eval_steps, nb_eval_examples = 0, 0\n","\n","  # Evaluate data for one epoch\n","  for batch in validation_dataloader:\n","    # Add batch to GPU\n","    batch = tuple(t.to(device) for t in batch)\n","    # Unpack the inputs from our dataloader\n","    b_input_ids, b_input_mask, b_labels = batch\n","    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n","    with torch.no_grad():\n","      # Forward pass, calculate logit predictions\n","      outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n","      logits = outputs.logits\n","      loss = outputs.loss\n","      val_loss_set.append(loss.item()) \n","    \n","    # Move logits and labels to CPU\n","    logits = logits.detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","\n","    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","    \n","    eval_accuracy += tmp_eval_accuracy\n","    nb_eval_steps += 1\n","    eval_loss += loss.item()\n","  print(\"Validation loss: {}\".format(eval_loss/len(validation_dataloader)))\n","  print(\"Validation Accuracy: {}\".format(eval_accuracy/len(validation_dataloader)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l6R1Y6BUvy5U"},"source":["model.save_pretrained(WORKSPACE + \"/Experiment/ALBERT_3\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C6gwsasuypPX"},"source":["plt.figure(figsize=(15,8))\n","plt.title(\"Training loss\")\n","plt.xlabel(\"Batch\")\n","plt.ylabel(\"Loss\")\n","plt.plot(train_loss_set, 'b-')\n","plt.plot(val_loss_set, 'g-')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3HFvuIFOScGo"},"source":["# Model Evaluation"]},{"cell_type":"code","metadata":{"id":"XkF6ggipiiFl"},"source":["prediction_inputs = torch.tensor(test_inputs)\n","prediction_masks = torch.tensor(test_masks)\n","prediction_labels = torch.tensor(test_labels) \n","\n","prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=BATCH_SIZE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FIxiazIfjvTQ"},"source":["# Prediction on test set\n","# Put model in evaluation mode\n","model.eval()\n","\n","# Tracking variables \n","predictions , true_labels = [], []\n","\n","tmp_test_accuracy = 0\n","test_accuracy = 0\n","nb_test_steps = 0\n","\n","# Predict \n","for batch in prediction_dataloader:\n","  # Add batch to GPU\n","  batch = tuple(t.to(device) for t in batch)\n","  # Unpack the inputs from our dataloader\n","  b_input_ids, b_input_mask, b_labels = batch\n","  # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n","  with torch.no_grad():\n","    # Forward pass, calculate logit predictions\n","    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n","    logits = outputs[0]\n","\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  \n","  # Store predictions and true labels\n","  predictions.append(logits)\n","  true_labels.append(label_ids)\n","\n","  tmp_test_accuracy = flat_accuracy(logits, label_ids)\n","    \n","  test_accuracy += tmp_test_accuracy \n","  nb_test_steps += 1\n","\n","print(\"Testing Accuracy: {}\".format(test_accuracy/nb_test_steps))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UWeWxbB1kVn9"},"source":["# Import and evaluate each test batch using Matthew's correlation coefficient\n","from sklearn.metrics import matthews_corrcoef\n","matthews_set = []\n","\n","for i in range(len(true_labels)):\n","  matthews = matthews_corrcoef(true_labels[i],\n","                 np.argmax(predictions[i], axis=1).flatten())\n","  matthews_set.append(matthews)\n","\n","# Flatten the predictions and true values for aggregate Matthew's evaluation on the whole dataset\n","flat_predictions = [item for sublist in predictions for item in sublist]\n","flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n","flat_true_labels = [item for sublist in true_labels for item in sublist]\n","\n","print(\"Matthew Score: \", matthews_corrcoef(flat_true_labels, flat_predictions))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8-bQzxAUZnq3"},"source":["# Classification report\n","report = classification_report(flat_true_labels, flat_predictions, zero_division=0)\n","print(report)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s-CDzO1FZt80"},"source":["# Confusion Matrix\n","cm = confusion_matrix(flat_true_labels, flat_predictions, labels=[0,1])\n","ConfusionMatrixDisplay(cm, display_labels=[\"non-offensive\",\"offensive\"]).plot(values_format='')"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check False Positives\n","df = pd.DataFrame()\n","df['predictions'] = flat_predictions\n","df['correct_labels'] = flat_true_labels\n","print(df.groupby(df['predictions']).count())\n","print(df.groupby(df['correct_labels']).count())\n","incorrect_predictions = df[df['predictions'] != df['correct_labels']]\n","\n","fp = incorrect_predictions[incorrect_predictions['predictions'] == 1].index.values\n","fn = incorrect_predictions[incorrect_predictions['predictions'] == 0].index.values\n","\n","print(\"#false positives: \" + str(len(fp)))\n","print(\"#false negatives \" + str(len(fn)))\n","\n","np.array(sentences)[fn]"],"metadata":{"id":"bI6WRFVHuE1z"},"execution_count":null,"outputs":[]}]}